{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    stratify makes sure training and test subjects have the same class label proportions as the input set\n",
    "    test_size is % of data to use for testing\n",
    "'''\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train) #Calculate std deviation and sample mean for all features\n",
    "X_train_std = sc.transform(X_train) #Scale train and test sets\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ppn = Perceptron(eta0=.1,random_state=1)\n",
    "ppn.fit(X_train_std,y_train)\n",
    "\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print(f'Accuracy: {100 * accuracy_score(y_test,y_pred):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' View decision regions for classifier '''\n",
    "def plot_decision_regions(X,y,classifier,test_idx=None,resolution=.2):\n",
    "\n",
    "    markers= ('o','s','^','v','<')\n",
    "    colors = ('red','blue','lightgreen','gray','cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    x1_min,x1_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    x2_min,x2_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "\n",
    "    xx1,xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),np.arange(x2_min,x2_max,resolution))\n",
    "\n",
    "    lab = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    plt.contourf(xx1,xx2,lab,alpha=.3,cmap=cmap)\n",
    "    plt.xlim(xx1.min(),xx1.max())\n",
    "    plt.ylim(xx2.min(),xx2.max())\n",
    "\n",
    "    for idx,cl in enumerate(np.unique(y)):\n",
    "\n",
    "        plt.scatter(\n",
    "            x=X[y==cl,0],\n",
    "            y=X[y==cl,1],\n",
    "            alpha=.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f'Class {cl}',\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "    if test_idx:\n",
    "        X_test,y_test = X[test_idx,:],y[test_idx]\n",
    "        plt.scatter(\n",
    "            X_test[:,0],\n",
    "            X_test[:,1],\n",
    "            c='none',\n",
    "            edgecolor='black',\n",
    "            alpha=1,\n",
    "            linewidth=1,\n",
    "            marker='o',\n",
    "            s=100,\n",
    "            label='Test Set'\n",
    "        )\n",
    "\n",
    "X_combined_std = np.vstack((X_train_std,X_test_std))\n",
    "y_combined = np.hstack((y_train,y_test))\n",
    "plot_decision_regions(\n",
    "    X=X_combined_std,\n",
    "    y=y_combined,\n",
    "    classifier=ppn,\n",
    "    test_idx=range(105,150)\n",
    ")\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron's major issue is that it never converges on datasets that aren't perfectly linerally seperable, meaning that we can't optimize it much more beyond what you see above. \n",
    "\n",
    "Another, more powerful option, is logistic regression. Logistic regression performs very well on linerally seperable datasets. \n",
    "\n",
    "The general premise of how LR works starts with the concept of odds, defined as P/(1-P). So, an event that has a 75% chance of happening would be 75/(100-75) -> 75/25 -> 3/1 or 3:1 odds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds(x):\n",
    "    return x / (1-x)\n",
    "plt.plot([odds(x) for x in np.arange(.01,1,.01)])\n",
    "plt.xlabel('Positive Event Probability (Percentage)')\n",
    "plt.ylabel('Event Odds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat our probabilites on a range of 0-1 (0% to 100%), we can then use the Logit function to expand those odds out to numbers across the entire real number specturm, defined as logit(p) = log(p/(1-p)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(odds(p))\n",
    "\n",
    "plt.plot([logit(x) for x in np.arange(.01,1,.01)])\n",
    "plt.axhline(0,color='k')\n",
    "plt.xlabel('Positive Event Probability')\n",
    "plt.ylabel('Event Odds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're close, but we need to be able to use this for values on the y axis and not the x axis for feature probabilty. To do this, we use the inverse logit function, known as the Logistic Sigmoid Function f(z) = 1/(1+e^-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # 1/ 1+e^-z\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-7,7,.1)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "plt.plot(z,sigma_z)\n",
    "plt.axvline(0,color='k')\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\sigma (z)$')\n",
    "plt.yticks([0,.5,1])\n",
    "ax = plt.gca()\n",
    "ax.yaxis.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major benefit to this over the thresholding used for the Perceptron is that we can also extract a estimation of the probability that a data record belongs to a specific class. This is used for things such as weather forcasting to determine not only if it's going to rain, but also how likely it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_1(z):\n",
    "    return -np.log(sigmoid(z))\n",
    "def loss_0(z):\n",
    "    return -np.log(1-sigmoid(z))\n",
    "\n",
    "z = np.arange(-10,10,.1)\n",
    "sigma_z = sigmoid(z)\n",
    "\n",
    "c1 = [loss_1(x) for x in z]\n",
    "plt.plot(sigma_z,c1,label='L(w,b) if y=1')\n",
    "\n",
    "c0 = [loss_0(x) for x in z]\n",
    "plt.plot(sigma_z,c0,linestyle='--',label='L(w,b) if y=0')\n",
    "\n",
    "plt.ylim(0.0,5.1)\n",
    "plt.xlim([0,1])\n",
    "\n",
    "plt.xlabel('$\\sigma(z)$')\n",
    "plt.ylabel('L(w,b)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression & Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "\n",
    "    '''\n",
    "        @param eta: learning rate\n",
    "        @param n_iter: total epochs\n",
    "        @param random_state = seed for random weight generation\n",
    "    '''\n",
    "    def __init__(self,eta=0.01,n_iter=50,random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    ''' \n",
    "        @param X: training data\n",
    "        @param y: target values for training\n",
    "    '''\n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0,scale=0.01,size=X.shape[1])\n",
    "        self.b_ = np.float_(0.)\n",
    "        self.losses_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y-output)\n",
    "            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * 2.0 * errors.mean()\n",
    "            loss = (\n",
    "                -y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))) / X.shape[0]\n",
    "            )\n",
    "            self.losses_.append(loss)\n",
    "        return self\n",
    "\n",
    "    def net_input(self,X):\n",
    "        return np.dot(X,self.w_) + self.b_\n",
    "    \n",
    "    def activation(self,z):\n",
    "        return 1. / (1. + np.exp(-np.clip(z,-250,250))) #Sigmoid\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= .5 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_01_subset = X_train_std[(y_train == 0) | (y_train == 1)]\n",
    "y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n",
    "lrgd = LogisticRegressionGD(eta=.3,n_iter=1000,random_state=1)\n",
    "lrgd.fit(X_train_01_subset,y_train_01_subset)\n",
    "\n",
    "plot_decision_regions(X=X_train_01_subset,y=y_train_01_subset,classifier=lrgd)\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=100.0,solver='lbfgs',multi_class='ovr')\n",
    "lr.fit(X_train_std,y_train)\n",
    "\n",
    "plot_decision_regions(X=X_combined_std,y=y_combined,classifier=lr,test_idx=range(105,150))\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X_test_std[:3,:]) # <- Probability each of the first three flowers belong to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Overfitting via Regularization (the C parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C is inversely proportional to the regularization paramter, therefore decreasing it's value means increasing the regularization strength as shown below\n",
    "#This graph shows only the weight coefficients for Iris-versicolor versus all classifiers. A weight coefficiennt approaching 0 leads to underfitting due to an \n",
    "#aggressive over regularization. So, paradoxically, where regularization can reduce over fit, over regularization can lead to underfit. \n",
    "\n",
    "weights,params= [],[]\n",
    "for c in np.arange(-5,5):\n",
    "    lr= LogisticRegression(C=10.**c,multi_class='ovr')\n",
    "    lr.fit(X_train_std,y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.**c)\n",
    "weights = np.array(weights)\n",
    "plt.plot(params,weights[:,0],label='Petal Length')\n",
    "plt.plot(params,weights[:,1],linestyle='--',label='Petal Width')\n",
    "plt.ylabel('Weight Coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear',C=1,random_state=1)\n",
    "svm.fit(X_train_std,y_train)\n",
    "\n",
    "plot_decision_regions(X_combined_std,y_combined,classifier=svm,test_idx=range(105,150))\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If our dataset is larger than memory, an alternative is the SGDClassifier, a concept similar to stochastic gradient descent\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "ppn = SGDClassifier(loss='perceptron') # <- loss=perceprton, log for logistic regression, and hinge for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVMs for nonlinear classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a synthetic data set for non-linear classification\n",
    "np.random.seed(1)\n",
    "\n",
    "X_xor = np.random.randn(200,2)\n",
    "y_xor = np.logical_xor(X_xor[:,0] > 0,X_xor[:,1] > 1)\n",
    "y_xor = np.where(y_xor,1,0)\n",
    "\n",
    "plt.scatter(\n",
    "    X_xor[y_xor == 1,0],\n",
    "    X_xor[y_xor == 1,1],\n",
    "    c='royalblue',\n",
    "    marker='s',\n",
    "    label='Class 1'\n",
    ")\n",
    "plt.scatter(\n",
    "    X_xor[y_xor == 0,0],\n",
    "    X_xor[y_xor == 0,1],\n",
    "    c='tomato',\n",
    "    marker='o',\n",
    "    label='Class 0'\n",
    ")\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-3,3])\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using the Radial Bias Function (RBF), aka Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',random_state=1,gamma=.1,C=10.0)\n",
    "svm.fit(X_xor,y_xor)\n",
    "\n",
    "plot_decision_regions(X_xor,y_xor,classifier=svm)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of RBF on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf',gamma=.2,random_state=1,C=1.0)\n",
    "svm.fit(X_train_std,y_train)\n",
    "plot_decision_regions(X_combined_std,y_combined,classifier=svm,test_idx=range(105,150))\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super high gamma here\n",
    "svm = SVC(kernel='rbf',gamma=100.0,random_state=1,C=1.0)\n",
    "svm.fit(X_train_std,y_train)\n",
    "plot_decision_regions(X_combined_std,y_combined,classifier=svm,test_idx=range(105,150))\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are easily interpretable as we can think of them as breaking our data down and classifying it using a series of questions. \\\n",
    "These questions are generated by starting at the root and splitting it and each subsequent node until all leaf nodes are pure, i.e they all belong \\\n",
    "to the same class. We determine which nodes to split based on the ones that provide the most Information Gain (IG) as those tend to be the most informative \\\n",
    "features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Impurity Measure\n",
    "\n",
    "This aims to maximize the mutual distribution of data, theremore it is largest when the distribution of class \\\n",
    "labels in a feature node is even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -p * np.log2(p) - (1-p) * np.log2((1-p))\n",
    "\n",
    "x = np.arange(0.0,1.0,0.01)\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "\n",
    "plt.ylabel('Entropy')\n",
    "plt.xlabel('Class Membership Probability p(i=1)')\n",
    "plt.plot(x,ent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Entropy, Gini, and Classification Impurity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "    return p*(1-p) + (1-p)*(1-(1-p))\n",
    "def error(p):\n",
    "    return 1-np.max([p,1-p])\n",
    "\n",
    "sc_ent = [e*.5 if e else None for e in ent]\n",
    "err = [error(i) for i in x]\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "for i,lab,ls,c in zip([ent,sc_ent,gini(x),err],['Entropy','Entropy (Scaled)','Gini Impurity','Misclassification Error'],['-','-','--','-.'],['black','lightgray','red','green','cyan']):\n",
    "    line = ax.plot(x,i,label=lab,linestyle=ls,lw=2,color=c)\n",
    "\n",
    "ax.legend(loc='upper center',bbox_to_anchor=(.5,1.15),ncol=5,fancybox=True,shadow=False)\n",
    "ax.axhline(y=.5,linewidth=1,color='k',linestyle='--')\n",
    "ax.axhline(y=1.0,linewidth=1,color='k',linestyle='--')\n",
    "plt.ylim([0,1.1])\n",
    "plt.xlabel('p(i=1)')\n",
    "plt.ylabel('Impurity Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Decision Tree in SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier(criterion='gini',max_depth=4,random_state=1)\n",
    "\n",
    "tree_model.fit(X_train,y_train)\n",
    "X_combined = np.vstack((X_train,X_test))\n",
    "y_combined = np.hstack((y_train,y_test))\n",
    "\n",
    "plot_decision_regions(X_combined,y_combined,classifier=tree_model,test_idx=range(105,150))\n",
    "plt.xlabel('Petal Length [cm]')\n",
    "plt.ylabel('Petal Width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Our Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "feature_names = ['Sepal Length','Sepal Width','Petal Length','Petal Width']\n",
    "tree.plot_tree(tree_model,feature_names=feature_names,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.findall(r'[a-zA-Z0-9]{4}','thisthatthose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {100000:1,'B':2,'C':3}\n",
    "d[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=25,random_state=1,n_jobs=2)\n",
    "forest.fit(X_train,y_train)\n",
    "\n",
    "plot_decision_regions(X_train,y_train,classifier=forest)\n",
    "plt.xlabel('Petal Length [cm]')\n",
    "plt.ylabel('Petal Width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5,p=2,metric='minkowski')\n",
    "knn.fit(X_train_std,y_train)\n",
    "\n",
    "plot_decision_regions(X_combined_std,y_combined,classifier=knn,test_idx=range(105,150))\n",
    "plt.xlabel('Petal Length [standardized]')\n",
    "plt.ylabel('Petal Length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Ensemble generalization score vs an Individual Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "import math\n",
    "\n",
    "def ensemble_error(n_classifier : int, error : float) -> float | int :\n",
    "\n",
    "    ''' \n",
    "        Calculates the error of an ensemble classifier. An assumption\n",
    "        is made that the ensemble's classifiers have equal error rates,\n",
    "        are independent of one another, and the errors are not correlated.\n",
    "\n",
    "        This function demonstrates the total error of the model when \n",
    "        n_classifer / 2 to n_classifiers miscalculate the class label.\n",
    "        In otherwords, the worst case scenario when an ensemble would\n",
    "        misclassify a lable using majority voting.\n",
    "\n",
    "        @param n_classifier: number of classifiers in the ensemble model\n",
    "        @param error       : error rate of each classifier in the model\n",
    "    '''\n",
    "\n",
    "    def instance_error(k : int, n_classifiers : int, error : float) -> float | int :\n",
    "        ''' \n",
    "            Calculate the probability of an ensemble model producing an error\n",
    "            given the total classifiers that produced misclassifications, the \n",
    "            error rate of each classifier, and the error for each classifer.\n",
    "\n",
    "            This is calculated as the binomial mass function of a binomial distribution.\n",
    "            \n",
    "            @param k: total classfiers that misclassified the data\n",
    "            @param n_classifers: total classifiers in the ensemble\n",
    "            @param error       : error rate of each classifier\n",
    "        '''\n",
    "\n",
    "        return comb(n_classifier,k) * (error**k) * ((1-error)**(n_classifier - k))\n",
    "    \n",
    "\n",
    "    k_start = int(math.ceil(n_classifier / 2.))\n",
    "    probs = [instance_error(k,n_classifier,error) for k in range(k_start,n_classifier+1)]\n",
    "    return sum(probs)\n",
    "\n",
    "ensemble_error(n_classifier=11, error= .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "    The error probability of an ensemble is always better than that of an individual classifier as long as the base classifiers\n",
    "    perform better than random guessing.\n",
    "'''\n",
    "error_range = np.arange(0.,1.01,.01)\n",
    "ensemble_errors = [ensemble_error(n_classifier=11,error=e) for e in error_range]\n",
    "\n",
    "''' Plot our error for an ensemble classifier'''\n",
    "plt.plot(\n",
    "    error_range,\n",
    "    ensemble_errors,\n",
    "    linewidth=2,\n",
    "    label='Ensemble Error'\n",
    ")\n",
    "\n",
    "''' Plot our error for a single classifier '''\n",
    "plt.plot(\n",
    "    error_range,\n",
    "    error_range,\n",
    "    linestyle='--',\n",
    "    label='Base Error',\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "plt.xlabel('Base Error')\n",
    "plt.ylabel('Base/Ensemble Error')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.grid(alpha=.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Majority Vote by label and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_by_label(labels : list[int], weights : list[int | float] = None) -> int :\n",
    "    ''' \n",
    "        Calculates the predicted label based on the mode, with consideration to weights if defined\n",
    "        @param labels             : list of class labels as predicted by the ensemble classifer\n",
    "        @param weights [optional] : weights to assign to predictions by each classifier\n",
    "    '''\n",
    "    return np.argmax(np.bincount(labels, weights=weights))\n",
    "\n",
    "majority_vote_by_label(labels = [0,0,1], weights=[.2,.2,.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote_by_probability(probabilities : list[list[int]], weights : list[int | float] = None) -> int :\n",
    "    '''\n",
    "        Calculates the predicted label of an ensemble based on class membership probabilities, with \n",
    "        consideration to weights if defined, by using the max weighted average for each class membership\n",
    "        along axis 0 and taking the max average.\n",
    "\n",
    "        Each probability list in probabilities should be [class 0 proba, class 1 proba]\n",
    "        and should add up to 1.\n",
    "\n",
    "        @param probabilites       : class membership probabilites, 2D array\n",
    "        @param weights [optional] : weights to assign to probabilities by each classifier\n",
    "    '''\n",
    "    weighted_avg = np.average(probabilities, weights=weights, axis=0)\n",
    "    return np.argmax(weighted_avg)\n",
    "\n",
    "majority_vote_by_probability(\n",
    "    probabilities= [\n",
    "        [.9,.1],\n",
    "        [.8,.2],\n",
    "        [.4,.6]\n",
    "    ],\n",
    "    weights=[.2,.2,.6]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Majority Vote Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import  Any, Self\n",
    "from sklearn.pipeline import _name_estimators\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator,ClassifierMixin,clone\n",
    "\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    ''' Majority Vote Classification Ensemble Model '''\n",
    "\n",
    "    def __init__(self, classifiers : list[Any], vote : str = 'classlabel', weights : list[int] = None) -> None :\n",
    "        \n",
    "        '''\n",
    "            @param classifiers: list of individual classifiers to use in the ensemble\n",
    "            @param vote       : voting strategy to use ['classlabel' or 'probabilities']\n",
    "            @param weights    : weights to use on predictions from each classifier when voting.\n",
    "        '''\n",
    "\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {k:v for k,v in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X : list[list[Any]], y : list[list[Any]]) -> Self :\n",
    "\n",
    "        ''' \n",
    "\n",
    "            Fit our ensemble classifiers to the training data \n",
    "            @param X : training data to train with\n",
    "            @param y : training data labels\n",
    "\n",
    "        '''\n",
    "        if self.vote not in ('probability','classlabel'):\n",
    "            raise ValueError(f'vote must be \\'probability\\' or \\'classlabel\\', got {self.vote}.')\n",
    "        \n",
    "        if self.weights and (len(self.weights) != len(self.classifiers)):\n",
    "            raise ValueError(f'Number of classifiers and weights must be equal. Got {len(self.weights)} weights and {len(self.classifiers)} classifiers.')\n",
    "        \n",
    "        ''' Encode class labels '''\n",
    "        self.labelenc_ = LabelEncoder()\n",
    "        self.labelenc_.fit(y)\n",
    "        self.classes_ = self.labelenc_.classes_\n",
    "\n",
    "        ''' Fit classifiers '''\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.labelenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _majority_vote_probas(self, X) -> list[int] :\n",
    "        ''' Calculate the majority vote class label using class membership probabilities '''\n",
    "        return np.argmax(self.predict_proba(X),axis=1)\n",
    "    \n",
    "    def __instance_vote(self, x) -> int :\n",
    "        ''' Calcualate a single classifiers predicted class label '''\n",
    "        return np.argmax(np.bincount(x,weights=self.weights))\n",
    "    \n",
    "    def _majority_vote_cls(self, X) -> list[int] :\n",
    "        ''' Calculate the majority vote class label using predicted class labels '''\n",
    "        preds = np.asarray([clf.predict(X) for clf in self.classifiers_]).T\n",
    "        return np.apply_along_axis(self.__instance_vote,axis=1,arr=preds)\n",
    "    \n",
    "    def predict(self, X) -> list[int] : \n",
    "\n",
    "        ''' \n",
    "            Predict class labels for testing data X using our fitted classifiers and majority vote\n",
    "            @param X : testing data\n",
    "        '''\n",
    "\n",
    "        #Don't remove this until you know the added code works\n",
    "        if self.vote == 'probability':\n",
    "            maj_vote = np.argmax(self.predict_proba(X),axis=1)\n",
    "        else:\n",
    "            predictions = np.asarray([clf.predict(X) for clf in self.classifers_]).T\n",
    "            maj_vote = np.apply_along_axis(lambda x : np.argmax(np.bincount(x,weights=self.weights)),axis=1,arr=predictions)\n",
    "\n",
    "        #maj_vote = self._majority_vote_probas(X) if self.vote == 'probability' else self._majority_vote_cls(X)\n",
    "        return self.labelenc_.inverse_transform(maj_vote)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        ''' Predict the class membership probabilites for our testing data '''\n",
    "        probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas,axis=1,weights=self.weights)\n",
    "        return avg_proba\n",
    "    \n",
    "    def get_params(self, deep: bool = True) -> dict:\n",
    "        \n",
    "        if not deep:\n",
    "            return super().get_params(deep)\n",
    "        \n",
    "        out = self.named_classifiers.copy()\n",
    "        for name,step in self.named_classifiers.items():\n",
    "            for k,v in step.get_params(deep=True).items():\n",
    "                out[f'{name}_{k}'] = v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X,y = iris.data[50:,[1,2]], iris.target[50:]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=.5, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l2', C=.001, solver='lbfgs', random_state=1)\n",
    "clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "\n",
    "pipe1 = Pipeline([['sc',StandardScaler()],['clf',clf1]])\n",
    "pipe3 = Pipeline([['sc',StandardScaler()],['clf',clf3]])\n",
    "\n",
    "clf_labels = ['Logistic Regression', 'Decision Tree', 'KNN']\n",
    "\n",
    "''' Predictive performance of each individual classifier calculation '''\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf,label in zip([pipe1,clf2,pipe3],clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "    print(f'ROC AUC: {scores.mean():<4.2f} ( +/- {scores.std():<4.2f} ) [{label:^24}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\n",
    "\n",
    "clf_labels += ['Majority Voting']\n",
    "all_clf = [pipe1, clf2, pipe3, mv_clf]\n",
    "\n",
    "for clf,label in zip(all_clf,clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "    print(f'ROC AUC: {scores.mean():<4.2f} ( +/- {scores.std():<4.2f} ) [{label:^24}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

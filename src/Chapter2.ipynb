{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    '''\n",
    "        @param eta: learning rate\n",
    "        @param n_iter: total epochs\n",
    "        @param random_state = seed for random weight generation\n",
    "    '''\n",
    "    def __init__(self,eta=0.01,n_iter=50,random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    ''' \n",
    "        @param X: training data\n",
    "        @param y: target values for training\n",
    "    '''\n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "\n",
    "        #Weights after fitting, X.shape[1] gives us the number of features\n",
    "        #We init with small random numbers to start\n",
    "        self.w_ = rgen.normal(loc=0.0,scale=0.01,size=X.shape[1]) \n",
    "        \n",
    "        self.b_ = np.float_(0.) #Bias after fitting\n",
    "        self.errors_ = [] #Number of miscalculations for each epoch\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi,target in zip(X,y):\n",
    "                update = self.eta * (target-self.predict(xi))\n",
    "\n",
    "                self.w_ += update * xi\n",
    "                self.b_ += update\n",
    "\n",
    "                errors += int(update != 0.0)\n",
    "\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self,X):\n",
    "        return np.dot(X,self.w_) + self.b_\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.where(self.net_input(X) >= 0.0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load Iris Data Set '''\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None,encoding='utf-8')\n",
    "\n",
    "y = data.iloc[0:100,4].values\n",
    "y = np.where(y == 'Iris-setosa',0,1)\n",
    "X = data.iloc[0:100,[0,2]].values\n",
    "\n",
    "''' Fit data to Perceptron'''\n",
    "ppn = Perceptron(eta=0.1,n_iter=10)\n",
    "ppn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' View Data Set '''\n",
    "plt.scatter(X[:50,0],X[:50,1],color='red',marker='o',label='Setosa')\n",
    "plt.scatter(X[50:100,0],X[50:100,1],color='blue',marker='s',label='Versicolor')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Petal Length (cm)')  \n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Error rate by Epoch '''\n",
    "plt.plot(range(1,len(ppn.errors_) + 1),ppn.errors_,marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('number of updates'  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' View decision regions for classifier '''\n",
    "def plot_decision_regions(X,y,classifier,resolution=.2):\n",
    "\n",
    "    markers= ('o','s','^','v','<')\n",
    "    colors = ('red','blue','lightgreen','gray','cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    x1_min,x1_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "    x2_min,x2_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "\n",
    "    xx1,xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),np.arange(x2_min,x2_max,resolution))\n",
    "\n",
    "    lab = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    plt.contourf(xx1,xx2,lab,alpha=.3,cmap=cmap)\n",
    "    plt.xlim(xx1.min(),xx1.max())\n",
    "    plt.ylim(xx2.min(),xx2.max())\n",
    "\n",
    "    for idx,cl in enumerate(np.unique(y)):\n",
    "\n",
    "        plt.scatter(\n",
    "            x=X[y==cl,0],\n",
    "            y=X[y==cl,1],\n",
    "            alpha=.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f'Class {cl}',\n",
    "            edgecolor='black'\n",
    "        )\n",
    "\n",
    "plot_decision_regions(X,y,ppn)  \n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Petal Length (cm)')  \n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADALINE (Adaptive Neural Neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD:\n",
    "\n",
    "    '''\n",
    "        @param eta: learning rate\n",
    "        @param n_iter: total epochs\n",
    "        @param random_state = seed for random weight generation\n",
    "    '''\n",
    "    def __init__(self,eta=0.01,n_iter=50,random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    ''' \n",
    "        @param X: training data\n",
    "        @param y: target values for training\n",
    "    '''\n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "\n",
    "        #Weights after fitting, X.shape[1] gives us the number of features\n",
    "        #We init with small random numbers to start\n",
    "        self.w_ = rgen.normal(loc=0.0,scale=0.01,size=X.shape[1]) \n",
    "        \n",
    "        self.b_ = np.float_(0.) #Bias after fitting\n",
    "        self.losses_ = [] #Number of miscalculations for each epoch\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y-output)\n",
    "\n",
    "            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * 2.0 * errors.mean()\n",
    "            loss = (errors**2).mean()\n",
    "\n",
    "            self.losses_.append(loss)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def net_input(self,X):\n",
    "        return np.dot(X,self.w_) + self.b_\n",
    "    \n",
    "    def activation(self,X):\n",
    "        return X # Linear Activation\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= .5 , 1, 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating two Adalines with varing learning rates to demonstrate tuning of hyperparameters'''\n",
    "ada1 = AdalineGD(n_iter=15,eta=0.1).fit(X,y)\n",
    "ada2 = AdalineGD(n_iter=15,eta=0.0001).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
    "\n",
    "''' Configure Plot for Ada1'''\n",
    "ax[0].plot(range(1,len(ada1.losses_)+1),\n",
    "           np.log10(ada1.losses_),marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Log(Mean Squared Error)')\n",
    "ax[0].set_title('Adaline - Learning Rate 0.1')\n",
    "\n",
    "''' Configure Plot for Ada2 '''\n",
    "ax[1].plot(range(1,len(ada2.losses_)+1),\n",
    "           np.log10(ada2.losses_),marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean Squared Error')\n",
    "ax[1].set_title('Adaline - Learning Rate 0.0001')\n",
    "\n",
    "print('Left Figure : Shows the overshooting over the Global Loss Minimum due to a learning rate that is much too large.')\n",
    "print('Right Figure: Shows a learning rate that is too small, this would need many epochs to reach the Global Loss Minimum.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Gradient Descent Optimization Via Standardization '''\n",
    "\n",
    "#Standardization of X\n",
    "X_std = np.copy(X)\n",
    "X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
    "X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
    "\n",
    "#Training Adaline model with a standardized X\n",
    "ada_gd = AdalineGD(n_iter=20,eta=0.5)\n",
    "ada_gd.fit(X_std,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plotting Decision Regions for Adaline with standardization '''\n",
    "plot_decision_regions(X_std,y,classifier=ada_gd)\n",
    "plt.title(\"Adaline - Gradient Descent\")\n",
    "plt.xlabel('Sepal Length [standardized]')\n",
    "plt.ylabel('Petal Length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot declining loss using standardization '''\n",
    "plt.plot(range(1,len(ada_gd.losses_)+1),\n",
    "         ada_gd.losses_,marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD:\n",
    "\n",
    "    '''\n",
    "        @param eta: learning rate\n",
    "        @param n_iter: total epochs\n",
    "        @param random_state = seed for random weight generation\n",
    "    '''\n",
    "    def __init__(self,eta=0.01,n_iter=50,shuffle=True,random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _initialize_weights(self,m):\n",
    "        ''' Initialize weights with random small numbers '''\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rgen.normal(loc=0.0,scale=0.01,size=m)\n",
    "        self.b_ = np.float_(0.)\n",
    "        self.w_initialized = True\n",
    "\n",
    "    def _shuffle(self,X,y):\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r],y[r]\n",
    "    \n",
    "    def _update_weights(self,xi,target):\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = (target-output)\n",
    "        self.w_ += self.eta * 2.0 * xi * (error)\n",
    "        self.b_ += self.eta * 2.0 * error\n",
    "        loss = error**2\n",
    "        return loss\n",
    "    \n",
    "    ''' \n",
    "        @param X: training data\n",
    "        @param y: target values for training\n",
    "    '''\n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.losses_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X,y = self._shuffle(X,y)\n",
    "\n",
    "            losses = []\n",
    "            for xi,target in zip(X,y):\n",
    "                losses.append(self._update_weights(xi,target))\n",
    "            avg_loss = np.mean(losses)\n",
    "            self.losses_.append(avg_loss)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def net_input(self,X):\n",
    "        return np.dot(X,self.w_) + self.b_\n",
    "    \n",
    "    def activation(self,X):\n",
    "        return X # Linear Activation\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= .5 , 1, 0)\n",
    "    \n",
    "    ''' Allows us to update our model on the fly as new data is added '''\n",
    "    def partial_fit(self,X,y):\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi,target in zip(X,y):\n",
    "                self._update_weights(xi,target)\n",
    "\n",
    "        else:\n",
    "            self._update_weights(X,y)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train our Adaline model using Stochastic Gradient Descent '''\n",
    "ada_sgd = AdalineSGD(n_iter=15,eta=0.01,random_state=1)\n",
    "ada_sgd.fit(X_std,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plotting Decision Regions for Adaline with standardization '''\n",
    "plot_decision_regions(X_std,y,classifier=ada_sgd)\n",
    "plt.title(\"Adaline - Stochastic Gradient Descent\")\n",
    "plt.xlabel('Sepal Length [standardized]')\n",
    "plt.ylabel('Petal Length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot declining loss using standardization '''\n",
    "plt.plot(range(1,len(ada_sgd.losses_)+1),\n",
    "         ada_sgd.losses_,marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download as nltk_download\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "MOVIE_DATA_LOCATION = Path('../data/movie_data.csv')\n",
    "\n",
    "if not nltk_download('stopwords', quiet=True):\n",
    "    raise Exception(\"Unable to download stopwords\")\n",
    "\n",
    "nltk_stop = stopwords.words('english')\n",
    "\n",
    "tqdm.pandas(leave=False)\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MOVIE_DATA_LOCATION.is_file():\n",
    "    ''' load and preprocess data if it hasn't been done already '''\n",
    "    \n",
    "    labels = {\n",
    "        'pos' : 1,\n",
    "        'neg' : 0\n",
    "    }\n",
    "\n",
    "    review_directories = ['test', 'train']\n",
    "\n",
    "    df = []\n",
    "    base_path = Path('../data/aclImdb/')\n",
    "    with tqdm(total=50_000, desc = 'Progress', leave = False, position = 0) as bar:\n",
    "\n",
    "        for review_directory in review_directories:\n",
    "            for sentiment_directory in labels.keys():\n",
    "                \n",
    "                path = base_path / review_directory / sentiment_directory\n",
    "                sentiment = labels[sentiment_directory]\n",
    "                \n",
    "                for file in sorted(path.iterdir()):\n",
    "                    \n",
    "                    with open(file, 'r', encoding='utf-8') as infile:\n",
    "                        review = infile.read()\n",
    "                        \n",
    "                    df.append(\n",
    "                        {'review' : review, 'sentiment' : sentiment}\n",
    "                    )\n",
    "                    \n",
    "                    bar.update()\n",
    "                    \n",
    "    df = pd.json_normalize(df)\n",
    "\n",
    "    ''' \n",
    "        Shuffle our movie data so class labels aren't sorted, so we can directly \n",
    "        stream the data from disk later on and have an easier time splitting\n",
    "        our data into test/train sets\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "    ''' save preprocessed data locally '''\n",
    "    df.to_csv(MOVIE_DATA_LOCATION, index=False, encoding='utf-8')\n",
    "\n",
    "df = pd.read_csv(MOVIE_DATA_LOCATION)\n",
    "\n",
    "assert df.shape == (50_000, 2) # sanity check to make sure our data is the proper size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "def preprocessor(text : str) -> str :\n",
    "    \n",
    "    def _replace_non_alphanumeric(text : str) -> str :\n",
    "        non_alnum_pattern = '[\\W]+'\n",
    "        return re.sub(non_alnum_pattern, ' ', text)\n",
    "    \n",
    "    def _replace_html_tags(text : str) -> str :\n",
    "        html_tag_pattern = r'<[^>]*>'\n",
    "        return re.sub(html_tag_pattern, '', text)\n",
    "    \n",
    "    def _rip_emoticons(text : str) -> tuple[str, list[str]] :  \n",
    "        ''' extract then remove emoticons, as a :D or :P without removal creates extra characters '''\n",
    "        emoticon_pattern = '(?::|;|=)(?:-)?(?:\\)|\\(|d|p)'\n",
    "        emoticons = re.findall(emoticon_pattern, text, flags=re.IGNORECASE) # save emoticons as those can be useful for sentiment\n",
    "        cleaned_text = re.sub(emoticon_pattern, '', text)\n",
    "        emoticons = ' '.join(emoticons).replace('-', '') # remove nose character from emoticons for consistency and join them together\n",
    "        return cleaned_text, emoticons\n",
    "        \n",
    "    text = text.lower()\n",
    "    text = _replace_html_tags(text) # remove html markup tags\n",
    "    text, emoticons = _rip_emoticons(text) # must extract emoticons before removing non alnums\n",
    "    text = _replace_non_alphanumeric(text) # remove non words from our text after making it lower case\n",
    "    \n",
    "    '''\n",
    "        since we are using a unigram approach, word order doesn't matter, so we can simply postpend our emoticons to the string.\n",
    "        In a 1+n gram model, word order WOULD be important\n",
    "    '''\n",
    "    return text + emoticons # cleaned string\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "@cache\n",
    "def _stem(text : str) -> str :\n",
    "    return porter.stem(text)\n",
    "\n",
    "def porter_tokenizer(text : str) -> list[str] :\n",
    "    return [_stem(x) for x in text.split()]\n",
    "\n",
    "def filter_stopwords(text : list[str]) -> list[str] :\n",
    "    return [x for x in text if x not in nltk_stop]\n",
    "\n",
    "def tokenizer(text : str) -> list[str] :\n",
    "    return text.split()\n",
    "\n",
    "df['preprocessed_review'] = df['review'].progress_map(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25_000, 'preprocessed_review'].values\n",
    "y_train = df.loc[:25_000, 'sentiment'].values\n",
    "X_test = df.loc[25_000:, 'preprocessed_review'].values\n",
    "y_test = df.loc[25_000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x0000017CA9AB3D80&gt;,\n",
       "                                              &lt;function porter_tokenizer at 0x0000017CA...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x0000017CA9AB3D80&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x0000017CA9AB3D80&gt;,\n",
       "                                              &lt;function porter_tokenizer at 0x0000017CA...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x0000017CA9AB3D80&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(lowercase=False)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x0000017CA9AB3D80>,\n",
       "                                              <function porter_tokenizer at 0x0000017CA...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x0000017CA9AB3D80>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "small_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range' :[(1,1)],\n",
    "        'vect__stop_words'   : [None],\n",
    "        'vect__tokenizer'    : [tokenizer, porter_tokenizer],\n",
    "        'clf__penalty'       : ['l2'],\n",
    "        'clf__C'              : [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range' :[(1,1)],\n",
    "        'vect__stop_words'   : [nltk_stop, None],\n",
    "        'vect__tokenizer'    : [tokenizer],\n",
    "        'vect__use_idf'      : [False],\n",
    "        'vect__norm'         : [None],\n",
    "        'clf__penalty'       : ['l2'],\n",
    "        'clf__C'              : [1.0, 10.0]\n",
    "    }\n",
    "]\n",
    "\n",
    "lr_tfidf = Pipeline(\n",
    "    [\n",
    "        ('vect', tfidf),\n",
    "        ('clf', LogisticRegression(solver='liblinear'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(\n",
    "    lr_tfidf, \n",
    "    small_param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5, \n",
    "    verbose=1, \n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set:  {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x0000017CA9AB3D80>}\n",
      "Cross-fold Validation Accuracy:  0.8972442391521696\n",
      "Test Accuracy :  0.89872\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: ', gs_lr_tfidf.best_params_)\n",
    "print('Cross-fold Validation Accuracy: ', gs_lr_tfidf.best_score_)\n",
    "print('Test Accuracy : ', gs_lr_tfidf.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative for large data sets using file streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', random_state=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stream_docs(path : Path) -> tuple[str, int] :\n",
    "    ''' Reads one document at a time '''\n",
    "    with open(path, 'r', encoding='utf-8') as infile:\n",
    "        next(infile)\n",
    "        \n",
    "        for line in infile :\n",
    "            review, sentiment = line[:-3], int(line[-2])\n",
    "            yield review, sentiment\n",
    "\n",
    "def get_minibatch(doc_stream, size : int) -> list[tuple[str, int]] :\n",
    "    reviews, sentiments = [], []\n",
    "    \n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            review, sentiment = next(doc_stream)\n",
    "            reviews.append(review)\n",
    "            sentiments.append(sentiment)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    \n",
    "    return reviews, sentiments\n",
    "\n",
    "vectorizer = HashingVectorizer(\n",
    "    decode_error='ignore',\n",
    "    n_features=2**21,\n",
    "    preprocessor=None,\n",
    "    tokenizer= lambda x : filter_stopwords(tokenizer(x))\n",
    ")\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
    "\n",
    "doc_stream = stream_docs(MOVIE_DATA_LOCATION)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "with tqdm(total=45, desc='Progress', leave=False, position=0) as bar:\n",
    "    \n",
    "    for _ in range(45):\n",
    "        X_train, y_train = get_minibatch(doc_stream, 1000)\n",
    "        if not X_train:\n",
    "            break\n",
    "        \n",
    "        X_train = vectorizer.transform(X_train)\n",
    "        clf.partial_fit(X_train, y_train, classes=classes)\n",
    "        \n",
    "        bar.update()\n",
    "        \n",
    "X_test, y_test = get_minibatch(doc_stream, 5000)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(f'Accuracy :', clf.score(X_test, y_test))\n",
    "\n",
    "clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "\n",
    "X = count_vectorizer.fit_transform(df['preprocessed_review'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=123, learning_method='batch')\n",
    "\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: \n",
      "  1. worst\n",
      "  2. comedy\n",
      "  3. script\n",
      "  4. awful\n",
      "  5. minutes\n",
      "Topic 2: \n",
      "  1. women\n",
      "  2. school\n",
      "  3. sex\n",
      "  4. girl\n",
      "  5. girls\n",
      "Topic 3: \n",
      "  1. music\n",
      "  2. song\n",
      "  3. songs\n",
      "  4. musical\n",
      "  5. dvd\n",
      "Topic 4: \n",
      "  1. family\n",
      "  2. father\n",
      "  3. wife\n",
      "  4. mother\n",
      "  5. woman\n",
      "Topic 5: \n",
      "  1. book\n",
      "  2. read\n",
      "  3. version\n",
      "  4. original\n",
      "  5. different\n",
      "Topic 6: \n",
      "  1. series\n",
      "  2. episode\n",
      "  3. episodes\n",
      "  4. tv\n",
      "  5. season\n",
      "Topic 7: \n",
      "  1. western\n",
      "  2. john\n",
      "  3. robert\n",
      "  4. murder\n",
      "  5. tom\n",
      "Topic 8: \n",
      "  1. role\n",
      "  2. performance\n",
      "  3. actor\n",
      "  4. performances\n",
      "  5. john\n",
      "Topic 9: \n",
      "  1. horror\n",
      "  2. budget\n",
      "  3. effects\n",
      "  4. killer\n",
      "  5. gore\n",
      "Topic 10: \n",
      "  1. action\n",
      "  2. game\n",
      "  3. fight\n",
      "  4. war\n",
      "  5. hero\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    \n",
    "    top_feature_names = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n",
    "    \n",
    "    print(f'Topic {topic_idx + 1}: ')\n",
    "    for idx, top_feature in enumerate(top_feature_names, 1):\n",
    "        print(f'  {idx}. {top_feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

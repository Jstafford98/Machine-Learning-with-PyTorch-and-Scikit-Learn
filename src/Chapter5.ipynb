{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compression Via Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "wine_cols = ['Class label','Alcohol','Malic Acid','Ash','Alcalinity of ash',\n",
    "             'Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols',\n",
    "             'Proanthocyanins','Color intensity','Hue',\n",
    "             'OD280/OD315 of diluted wines','Proline'\n",
    "            ]\n",
    "df_wine = pd.read_csv(url,header=None)\n",
    "df_wine.columns = wine_cols\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Dimensionality Reduction via Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,y = df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Standardize the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Construct a covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_train_std.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the Eigenpairs for the Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals,eigen_vecs = np.linalg.eig(cov_mat)\n",
    "print('\\nEigenvalues \\n',eigen_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphing the Variance Explained Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i/tot) for i in sorted(eigen_vals,reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(1,14),var_exp,align='center',label='Individual Explained Variance')\n",
    "plt.step(range(1,14),cum_var_exp,where='mid',label='Cumulative Explained Variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Final Feature Transformations onto the new principal component axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort (eigenvalue,eigenvector) from high to low\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]),eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "eigen_pairs.sort(key=lambda k : k[0],reverse=True)\n",
    "\n",
    "#Collect the two largest eigenvectors correlating to the two largest eigenvalues\n",
    "#This creates a 13x2 dim projection matrix\n",
    "w = np.hstack(\n",
    "    (\n",
    "        eigen_pairs[0][1][:,np.newaxis],\n",
    "        eigen_pairs[1][1][:,np.newaxis]\n",
    "    )\n",
    ")\n",
    "print('Matrix W:\\n',w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform our training data onto the PCA subspace\n",
    "X_train_pca = X_train_std.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the newly projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r','b','g']\n",
    "markers = ['o','s','^']\n",
    "\n",
    "for l,c,m in zip(np.unique(y_train),colors,markers):\n",
    "    plt.scatter(X_train_pca[y_train==l,0],\n",
    "                X_train_pca[y_train==l,1],\n",
    "                c=c,\n",
    "                label=f'Class {l}',\n",
    "                marker=m\n",
    "                )\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Dimensionality Reduction via Principal Component Analysis (Implemented using Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X,y,classifier,test_idx=None,resolution=.02):\n",
    "\n",
    "    #setup marker generator and color map\n",
    "    markers = ('o','s','^','v','<')\n",
    "    colors = ('red','blue','lightgreen','gray','cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    #plot decision surface\n",
    "    x1_min,x1_max = X[:,0].min()-1,X[:,0].max()+1\n",
    "    x2_min,x2_max = X[:,1].min()-1,X[:,1].max()+1\n",
    "    xx1,xx2 = np.meshgrid(\n",
    "        np.arange(x1_min,x1_max,resolution),\n",
    "        np.arange(x2_min,x2_max,resolution)\n",
    "    )\n",
    "\n",
    "    lab = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1,xx2,lab,alpha=.3,cmap=cmap)\n",
    "    plt.xlim(xx1.min(),xx1.max())\n",
    "    plt.ylim(xx2.min(),xx2.max())\n",
    "\n",
    "    #Plot Class Examples\n",
    "    for idx,cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x=X[y==cl,0],\n",
    "            y=X[y==cl,1],\n",
    "            alpha=.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f'Class {cl}',\n",
    "            edgecolor='black',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression(multi_class='ovr',random_state=1,solver='lbfgs')\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.fit_transform(X_test_std)\n",
    "\n",
    "lr.fit(X_train_pca,y_train)\n",
    "\n",
    "plot_decision_regions(X_train_pca,y_train,classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca*-1,y_test,classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to see the explained variance ratio of our principal components, we just need to init pca like this\n",
    "pca2 = PCA(n_components=None)\n",
    "X_train_pca = pca2.fit_transform(X_train_std)\n",
    "pca2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Feature Contributions to each principal component via factor loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = eigen_vecs * np.sqrt(eigen_vals)\n",
    "fig,ax = plt.subplots()\n",
    "ax.bar(range(13),loadings[:,0],align='center')\n",
    "ax.set_ylabel('Loadings for PC 1')\n",
    "ax.set_xticks(range(13))\n",
    "ax.set_xticklabels(df_wine.columns[1:],rotation=90)\n",
    "plt.ylim([-1,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Positive loadings implies a positive correlation and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_loadings = (pca.components_.T * np.sqrt(pca.explained_variance_))*-1\n",
    "fig,ax = plt.subplots()\n",
    "ax.bar(range(13),sklearn_loadings[:,0],align='center')\n",
    "ax.set_ylabel('Loadings for PC 1')\n",
    "ax.set_xticks(range(13))\n",
    "ax.set_xticklabels(df_wine.columns[1:],rotation=90)\n",
    "plt.ylim([-1,1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Data Compression via Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of Scatter Matricies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Mean Vectors for each class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "mean_vecs = []\n",
    "for label in range(1,4):\n",
    "    mean_vecs.append(\n",
    "        np.mean(\n",
    "            X_train_std[y_train==label],axis=0\n",
    "        )\n",
    "    )\n",
    "    print(f'MV {label}: {mean_vecs[label-1]}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute within class scatter matricies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 13 #Number of features\n",
    "S_W = np.zeros((d,d))\n",
    "for label,mv in zip(range(1,4),mean_vecs):\n",
    "    class_scatter = np.zeros((d,d))\n",
    "    for row in X_train_std[y_train == label]:\n",
    "        row,mv = row.reshape(d,1),mv.reshape(d,1)\n",
    "        class_scatter += (row-mv).dot((row-mv).T)\n",
    "    S_W+=class_scatter\n",
    "print('Within-class Scatter Matrix:',f'{S_W.shape[0]}x{S_W.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One assumption that is made when calculating the scatter matricies is that our class labels are uniformly-distributed, however that assumption is violated\n",
    "print('Class Label Distribution:',np.bincount(y_train)[1:])\n",
    "\n",
    "# Thus, we need to scale the individual scatter matricies before summing them up as the scatter matrix\n",
    "d = 13 \n",
    "S_W = np.zeros((d,d))\n",
    "for label,mv in zip(range(1,4),mean_vecs):\n",
    "    class_scatter = np.cov(\n",
    "        X_train_std[y_train == label].T\n",
    "    )\n",
    "    S_W+= class_scatter\n",
    "print('Within-class Scatter Matrix:',f'{S_W.shape[0]}x{S_W.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer Between-class Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_overall = np.mean(X_train_std,axis=0)\n",
    "mean_overall = mean_overall.reshape(d,1)\n",
    "\n",
    "d = 13\n",
    "S_B = np.zeros((d,d))\n",
    "for i, mean_vec in enumerate(mean_vecs):\n",
    "    n = X_train_std[y_train == i+1,:].shape[0]\n",
    "    mean_vec = mean_vec.reshape(d,1)\n",
    "    S_B += (n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T))\n",
    "print('Between-class Scatter Matrix:',f'{S_B.shape[0]}x{S_B.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Linear Discriminants for the New Feature Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals,eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]),eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0],reverse=True)\n",
    "print('Eigenvalues in descending order:\\n')\n",
    "for eigen_val in eigen_pairs:\n",
    "    print(eigen_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of Discriminability for each linear discriminate for each class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals.real)\n",
    "discr = [(i/tot) for i in sorted(eigen_vals.real,reverse=True)]\n",
    "cum_discr = np.cumsum(discr)\n",
    "plt.bar(range(1,14),discr,align='center',label='Individual Discriminability')\n",
    "plt.step(range(1,14),cum_discr,where='mid',label='Cumulative Discriminability')\n",
    "plt.ylabel('Discriminability Ratio')\n",
    "plt.xlabel('Linear Discriminants')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the first two discriminants capture 100% of the feature space, we need to select those\n",
    "w = np.hstack(\n",
    "    (\n",
    "        eigen_pairs[0][1][:,np.newaxis].real,\n",
    "        eigen_pairs[1][1][:,np.newaxis].real\n",
    "    )\n",
    ")\n",
    "print('Matrix W:\\n',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting our examples onto the new feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda = X_train_std.dot(w)\n",
    "colors = ['r','b','g']\n",
    "markers = ['o','s','^']\n",
    "\n",
    "for l,c,m in zip(np.unique(y_train),colors,markers):\n",
    "    plt.scatter(\n",
    "        X_train_lda[y_train==l,0],\n",
    "        X_train_lda[y_train==l,1]*(-1),\n",
    "        c=c,label=f'Class {l}',marker=m\n",
    "    )\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Our classes are now perfectly linearly seperable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Data Compression via Linear Discriminant Analysis (Implemented using Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std,y_train)\n",
    "\n",
    "lr = LogisticRegression(multi_class='ovr',random_state=1,solver='lbfgs')\n",
    "lr = lr.fit(X_train_lda,y_train)\n",
    "plot_decision_regions(X_train_lda,y_train,classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is 100% accurate on our 2D dataset, instead of our 13D dataset, thanks to LDA\n",
    "X_test_lda = lda.transform(X_test_std)\n",
    "plot_decision_regions(X_test_lda,y_test,classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "fig,ax = plt.subplots(1,4)\n",
    "for i in range(4):\n",
    "    ax[i].imshow(digits.images[i],cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_digits = digits.target\n",
    "X_digits = digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2,init='pca',random_state=123)\n",
    "X_digits_tsne = tsne.fit_transform(X_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as PathEffects\n",
    "def plot_projection(x,colors):\n",
    "    f = plt.figure(figsize=(8,8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    for i in range(10):\n",
    "        plt.scatter(\n",
    "            x[colors==i,0],\n",
    "            x[colors==i,1]\n",
    "        )\n",
    "    \n",
    "    for i in range(10):\n",
    "        xtext,ytext = np.median(x[colors==i,:],axis=0)\n",
    "        txt = ax.text(xtext,ytext,str(i),fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5,foreground='w'),\n",
    "            PathEffects.Normal()\n",
    "        ])\n",
    "plot_projection(X_digits_tsne,y_digits)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
